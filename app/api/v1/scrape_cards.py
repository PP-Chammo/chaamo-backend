from fastapi import APIRouter, HTTPException, Query
from httpx import HTTPStatusError, RequestError
from bs4 import BeautifulSoup

from app.models.card import ScrapeResponse, Region, SearchPage
from app.utils.fetcher import fetch_html
from app.handlers.scrape_cards_handler import process_scrape_cards
from app.utils.scrape_helpers import save_debug_html
from app.utils.supabase_helpers import save_to_supabase

router = APIRouter()

@router.get("/scrape_cards", response_model=ScrapeResponse, summary="Scrape eBay sold listings")
async def scrape_cards_endpoint(
    query: str = Query(
        "",
        description="Enter the name of the card you want to search for on eBay. For example: '2023 Topps Merlin Lamine Yamal'"
    ),
    region: Region = Query(
        Region.uk,
        description="Choose a region from 'us' or 'uk'"
    ),
    page: SearchPage = Query(
        SearchPage.one,
        description="Choose a page from search"
    )
):
    """
    Endpoint to scrape data of sold cards from eBay, save it to Supabase,
    and return the results.
    """
    print(f"\nğŸš€ Starting scrape_cards endpoint")
    print(f"   ğŸ“ Query: '{query}'")
    print(f"   ğŸŒ Region: {region.value}")
    print(f"   ğŸ“„ Page: {page.value}")
    
    if not query or not query.strip():
        print("âŒ ERROR: Empty or invalid query parameter")
        raise HTTPException(status_code=400, detail="Query parameter is required and cannot be empty")
    
    try:
        print("ğŸŒ Fetching HTML from eBay...")
        html_text = await fetch_html(query=query, region=region.value, page=page.value)
        print(f"   âœ… HTML fetched successfully ({len(html_text)} characters)")
        
    except HTTPStatusError as e:
        print(f"âŒ HTTP ERROR: {e.response.status_code} - {e.response.reason_phrase}")
        raise HTTPException(
            status_code=e.response.status_code,
            detail=f"Error fetching data from eBay: {e.response.reason_phrase}"
        )
    except RequestError as e:
        print(f"âŒ REQUEST ERROR: {e}")
        raise HTTPException(status_code=504, detail="Timeout or network error connecting to eBay.")

    if not html_text:
        print("âŒ ERROR: Empty HTML content received")
        raise HTTPException(status_code=500, detail="Failed to retrieve HTML content.")

    try:
        print("ğŸ”§ Processing scraped data...")
        grouped_cards = await process_scrape_cards(html_text)

        if not grouped_cards:
            print("âŒ ERROR: No grouped cards returned from processing")
            soup = BeautifulSoup(html_text, 'lxml')
            items = soup.select('li.s-item, li.s-card')
            print(f"   ğŸ” Found {len(items)} potential items in HTML")
            save_debug_html(str(items))
            raise HTTPException(status_code=500, detail="Failed to parse results.")

        print(f"âœ… Successfully processed {len(grouped_cards)} card groups")
        
        # Save to Supabase
        print("ğŸ’¾ Saving to Supabase...")
        await save_to_supabase(grouped_cards)
        print("âœ… Supabase save operation completed")

        print(f"ğŸ‰ Scrape operation successful - returning {len(grouped_cards)} card groups")
        return ScrapeResponse(result=grouped_cards)
        
    except Exception as e:
        print(f"âŒ UNEXPECTED ERROR during processing: {e}")
        soup = BeautifulSoup(html_text, 'lxml')
        items = soup.select('li.s-item, li.s-card')
        print(f"   ğŸ” Found {len(items)} potential items in HTML for debugging")
        save_debug_html(str(items))
        print(f"   ğŸ“ Full error details:")
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail="An internal error occurred while parsing the data.")
